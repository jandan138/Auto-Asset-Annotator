# 序章：赋予机器“视觉”的魔法

欢迎来到 **Auto-Asset-Annotator** 的世界！如果你从未接触过大模型（LLM），也不用担心。把这本手册当成一本冒险指南，我们将一步步探索如何用最新的 AI 技术来解决现实世界的问题。

## 我们的任务：整理巨型仓库

想象一下，你接手了一个巨大的数字仓库。这个仓库里堆满了成千上万个 3D 模型——有椅子、桌子、花瓶、还有各种奇怪的怪兽。

老板给了你一个任务：**“给这里的每一个物品都写一份详细的报告。包括它叫什么、是什么材质、有多大、放在哪里合适。”**

如果你一个人干，可能需要几十年才能写完。这时候，你需要一个帮手。

## 遇见“魔法大脑”：VLM

我们请来的帮手叫 **VLM (Vision-Language Model，视觉-语言模型)**。

*   **以前的 AI**：要么只能“看图”（比如识别这是一只猫），要么只能“聊天”（比如 ChatGPT）。
*   **VLM**：它既长了眼睛，又长了嘴巴。你可以给它看一张图，然后问它：“这张图里是什么？它的材质看起来硬吗？适合放在客厅吗？”

在这个项目中，我们使用的核心大脑是 **Qwen-VL (通义千问视觉版)**。它就像一个知识渊博的鉴定师，看过无数的图片和书，能一眼认出“这是一把维多利亚风格的红木椅子”。

## 什么是 Auto-Asset-Annotator？

如果 Qwen-VL 是鉴定师，那 **Auto-Asset-Annotator** 就是**鉴定中心**。

Qwen-VL 本身只能一次回答一个问题。而我们的工具（Auto-Asset-Annotator）做的是：
1.  **自动排队**：把仓库里的 3D 模型一个个拿出来，拍好照片。
2.  **自动提问**：拿着照片问 Qwen-VL，“这是什么？”、“有什么特点？”。
3.  **自动记录**：把 Qwen-VL 的回答整理成整齐的表格（JSON 文件）。

## 为什么这很酷？

*   **不知疲倦**：它可以在你睡觉时工作，一晚上处理几万个模型。
*   **多角度观察**：它不光看正面，还会看侧面、背面，然后综合所有信息给出一个结论（这比只看一眼准确多了！）。
*   **听得懂人话**：你不需要写复杂的代码，只需要用英语（或中文）告诉它你想知道什么（这就是 Prompt/提示词）。

准备好了吗？翻开下一章，我们要开始搭建属于你的魔法工坊了！
