# 第一章：搭建你的魔法工坊

在开始施法之前，我们需要先搭建好工作环境。就像哈利波特需要魔杖和坩埚一样，我们需要**GPU**和**Python环境**。

## 1. 核心能量源：GPU (显卡)

VLM 模型是一个庞然大物，它需要大量的计算能力。普通的 CPU（电脑的大脑）算得太慢了，我们需要 GPU（显卡）来加速。

*   **你需要什么**：一台装有 NVIDIA 显卡的电脑或服务器。
*   **显存 (VRAM)**：这就像工作台的大小。
    *   **16GB**：勉强够用，可以运行 7B 大小的模型（相当于一个聪明的初中生）。
    *   **24GB+**：推荐配置，干活更利索。
    *   **80GB**：土豪配置，可以运行 72B 模型（相当于博士生）。

## 2. 魔法结界：Conda

在编程世界里，不同的软件喜欢打架。为了不让它们打架，我们用 **Conda** 创建一个个独立的“房间”（虚拟环境）。

打开你的终端（Terminal），输入以下咒语（命令）：

```bash
# 创建一个叫 annotator 的新房间，里面装 Python 3.10
conda create -n annotator python=3.10 -y

# 走进这个房间
conda activate annotator
```

当你看到命令行前面出现 `(annotator)` 字样时，说明你已经进入了魔法结界。

## 3. 准备工具箱：安装依赖

进入我们的项目目录，然后安装需要的工具包：

```bash
cd /path/to/Auto-Asset-Annotator  # 走到项目文件夹

# 安装 requirements.txt 里列出的所有工具
pip install -r requirements.txt

# 把当前项目安装好，方便随时调用
pip install -e .
```

这一步可能会花点时间，就像是在整理工具箱，把扳手、螺丝刀都摆好。

## 4. 召唤守护灵：下载模型

我们需要把 Qwen-VL 模型下载到本地。这就像是去图书馆把百科全书借回来。

由于模型文件很大（几十 GB），我们通常会设置一个专门的存放路径：

```bash
# 告诉电脑：把模型存到这个大硬盘里！
export HF_HOME=/data/shared/huggingface
```

然后，当你第一次运行程序时，它会自动开始下载。如果你在受限的网络环境（比如公司内网），可能需要手动下载，具体可以参考 `docs/installation/linux_deployment.md`。

---

**工坊检查清单**：
- [ ] 显卡驱动装好了吗？（运行 `nvidia-smi` 看看）
- [ ] Conda 环境激活了吗？（看到 `(annotator)` 了吗？）
- [ ] 依赖包安装完了吗？（没有红色的 Error 吧？）

一切就绪？太棒了！下一章我们去准备原材料。
